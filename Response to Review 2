Response to Review 2: 

Thank you for your efforts and comments. We appreciate your input.  

>> Weakness (Theoretical foundations and reference to previous work - Non-overlapping users): Please note that in the literature MDR encompasses several scenarios, including overlapping and non-overlapping users and items across different domains - e.g. see [7,8,9]. Our AdapterSoup model, which does not (necessarily) require the users to overlap across the source and target domains (typically considered to be a good thing in the MDR literature since it eases the deployability of the model in real scenarios), still fits within the broader MDR framework as it aims to learn universal item representations across multiple domains. Our approach can work with both overlapping and non-overlapping users. In fact, there is 8.3% overlap of users across the used Amazon datasets. What we meant by “we do not require overlapping users” is that we do not (necessarily) need overlapping users to deploy the approach. In our evaluation, we report the performance of our method not only on the overlapping users but actually on all users within the target domain. We will clarify the issue in the paper to avoid any confusion.

>> Weakness (Theoretical foundations and reference to previous work - Chosen name and clarification of terminology): With the proliferation of acronyms in this neural networks era, it is hard sometimes to ensure that a relevant acronym is unique. We note however the point of the reviewer, and to avoid confusion with another model, we suggest renaming our model to AdapterSoupRec. We will also correct the mentioned references.

>> Weakness (Research Methods and Evidence Analysis):
[Overlapping users]. As clarified above, AdapterSoup is an approach that works with both overlapping and non-overlapping users. It uses MLLMs to encode universal item representations across multiple domains. To answer the reviewer’s question, the average percentage of overlapping users is 8.3% across the used datasets.
[Fusion methods] There are architectural differences between CLIP and LLaVA. While CLIP allows for mid-fusion, LLaVA only necessitates late-fusion since it inputs the encoded visual feature to the LLM (e.g. vicuna) along with textual token embeddings at a later stage. We have done this mid-fusion on CLIP in some preliminary experiments and it shows that there is no significant difference between late and mid-fusions.
[Potential combination with existing baselines]. Indeed, we have done the analysis of integrating the baselines with the used MLLMs as adapters. Our analysis shows that this integration significantly improved most of the baselines. However, due to the page length constraint, we cannot include all these experiments, especially since they go beyond the scope of validating the main claim of this paper, namely that the model soup technique does enhance the MLLMs in MDR. Page length constraints permitting, we can try to summarise this aspect in the paper.
[GCNext] Given that this is a short paper, and that both AdapterSoup and the used baselines address the top-k recommendation task, we do not think that it is necessary or useful to include a sequential recommender in the current version of the paper.
[Use of modalities in baselines] We ensure a fair comparison between AdapterSoup and all used baselines (esp. the multi-modal baselines) since we consistently use the same feature extractors for encoding multi-modal item features across all multi-modal baselines. Regarding the fusion methods, each baseline uses different fusion strategies based on the extracted visual and textual features. Please refer to the detailed feature extraction scripts in the provided anonymous GitHub repository.
[Dimentionality of multi-modal item embeddings] As a conventional practice, all baselines use 128 dimensions for user/item ID embeddings [1,2,3]. Typically, the multi-modal baselines combine these 128-dimensional visual and textual embeddings to construct the final item embeddings, which are then used to compute ranking scores via a dot product. We follow this same common practice and project the dimension of the extracted item embedding to 128, instead of using the 768-dimensional features which contain more abundant semantic information. In fact, we observed an improved effectiveness using the dimension of 768. We plan to summarise this finding in the next version of the paper.
[Flaws of terminology] We acknowledge the need to tidy up terminology and define our terms more precisely. We will clearly differentiate between 'trained' and 'fine-tuned' in the paper.
[Clarification of the embeddings used in the MLLMs] Both CLIP and LLaVA are inputted with item images and descriptions. Specifically, LLaVA encodes the item images and descriptions using a pre-trained CLIP visual encoder (i.e., ViT/32)  and an LLM (i.e., Vicuna), respectively. We will clarify the use of these embeddings within our model in the paper.
[MLLM’s loss function and user embeddings] This is indeed an oversight on our part. We will include a detailed description of the weighted term used in balancing the loss components in our MLLMs and how we use the obtained item embeddings to represent the user embeddings within the graph adapter. 
[Effectiveness of adapter]. Note that our approach, which combines soup-enhanced MLLMs with an adapter, has been shown in our experiments using 3 different datasets to be effective in enhancing the MDR performance. 
[Limitations and extensions] Please bear in mind this is a short paper. We do not have a lot of space in such papers. Page length constraints permitting, we try to briefly cover limitations and future work in the camera-ready version.
[Trainining of MLLM] We would like to clarify that we used the released versions of the MLLMs rather than training them from scratch. We will explicitly state this in the camera-ready version.

>> Weakness (Reproducibility): We have now provided a link to an anonymous GitHub repository with all required code and scripts (see beginning of this rebuttal response).

>> Weakness (Effectiveness of Communication): This is a good suggestion. We will address the issue as suggested by providing an explicit list of used parameters in the revised version. Regarding Equation (1), we intend to retain it to demonstrate how the visual and textual features are encoded from the raw item images and descriptions. Additionally,  we will refine the “soup ingredients” and l_{\alpha} to ensure that our descriptions are clearer, thereby reducing any potential misunderstandings.

>> Weakness (Minor Comments): We will give the paper another proofread including checking the notations and identifying any remaining typos. 
