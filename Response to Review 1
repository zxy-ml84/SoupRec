—---- Response to Review 1 —-----

Thank you for your feedback.

>> Weakness (a) -  (Novelty of the Paper): Thanks for your comment. However, we respectfully disagree with your comment. Indeed, in this paper, we are the first to apply the model soup technique on MLLMs so as to model universal item representations in the multi-domain recommendation (MDR) task. We are happy to stand corrected if the reviewer provides a supporting reference where the model soup technique has been tailored to MDR.  In the recommender systems research community, it is a common practice to train the models multiple times to select the best performing model. Our AdapterSoup model adheres to this convention but the soup technique allows us to integrate multiple model training configurations without adding extra parameters, thereby maintaining the same computational and parameter efficiency while enhancing the model’s performance. Note that we have covered the maintained model size in Section 3.3.

>> Weakness (b) - (Dataset Processing): We respectfully disagree with the comment regarding the information leakage from our random splitting of the train, validation, and test sets. First, this random splitting, is very commonly used by many recent recommenders like LATTICE [1] and BM3 [2] (and in fact the RecSys literature is full of such random data splits), and ensures our results are directly comparable to the related published work on the same datasets. Second, if the reviewer is generally skeptical about RecSys evaluation then it is a good point for general discussion, but we do not think that raising it against a short paper is fair or useful. Our experimental setup is quite standard, and it ensures that there is no training on the validation or test sets. We also provide all preprocessing scripts alongside our source code in the anonymous repository.


>> Weakness (c) - (Used Datasets): Thank you for the comment. We do appreciate the importance of using diverse datasets in evaluating the recommendation effectiveness. However, we have chosen the Amazon review datasets primarily because they are one of the few public datasets, which provide comprehensive raw multi-modal data across various recommendation domains. 
In contrast, the MovieLens dataset, for example, in addition to user-item ratings, mostly contains movie posters, which offer limited semantic information about the movies [4] while lacking any cross-domain scenarios. 
Note also that the Amazon datasets have been examined by several of the existing SOTA baselines [1, 2, 3]. Therefore, our selection of the Amazon review datasets was very reasonable, aiming to leverage their comprehensive raw data for a more effective evaluation of multi-domain recommendations, while also permitting the direct comparison of our results to the published literature. In general, for a short paper using 3 datasets seems entirely reasonable. We are happy to add generalisation to other datasets (if and when they become available) as a possible future work. 
>> Question 1: “How does the proposed approach compared with RecSys trained on the target domain?” 
Response: In our paper, we already directly compare our proposed AdapterSoup approach with existing recommender systems specifically trained on the target domain, such as VBPR, MMGCL, LATTICE, and BM3. These models are the current benchmarks in the field and align closely with our objective of assessing the model soup technique within MLLM for the MDR task. Furthermore, note that we have provided a detailed performance comparison between our AdapterSoup and these particular baselines in Table 2 of our paper, demonstrating the effectiveness of our novel AdaperSoup approach.
